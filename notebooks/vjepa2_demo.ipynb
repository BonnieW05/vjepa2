{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V-JEPA 2 Demo Notebook\n",
        "\n",
        "This tutorial provides an example of how to load the V-JEPA 2 model in vanilla PyTorch and HuggingFace, extract a video embedding, and then predict an action class. For more details about the paper and model weights, please see https://github.com/facebookresearch/vjepa2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's import the necessary libraries and load the necessary functions for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "if os.getcwd().endswith('notebooks'):\n",
        "    # 切换到项目根目录\n",
        "    os.chdir('..')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "\n",
        "import src.datasets.utils.video.transforms as video_transforms\n",
        "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
        "from src.models.attentive_pooler import AttentiveClassifier\n",
        "from src.models.vision_transformer import vit_large_rope\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 encoder\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 classifier\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def build_pt_video_transform(img_size):\n",
        "    short_side_size = int(256.0 / 224 * img_size)\n",
        "    # Eval transform has no random cropping nor flip\n",
        "    eval_transform = video_transforms.Compose(\n",
        "        [\n",
        "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
        "            video_transforms.CenterCrop(size=(img_size, img_size)),\n",
        "            volume_transforms.ClipToTensor(),\n",
        "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
        "        ]\n",
        "    )\n",
        "    return eval_transform\n",
        "\n",
        "\n",
        "# def get_video():\n",
        "#     vr = VideoReader(\"sample_video.mp4\")\n",
        "#     # choosing some frames here, you can define more complex sampling strategy\n",
        "#     frame_idx = np.arange(0, 128, 2)\n",
        "#     video = vr.get_batch(frame_idx).asnumpy()\n",
        "#     return video\n",
        "\n",
        "def get_video():\n",
        "    \"\"\"完全避开 torch.from_numpy()，使用 torch.tensor()\"\"\"\n",
        "    cap = cv2.VideoCapture(\"sample_video.mp4\")\n",
        "    \n",
        "    # 获取视频信息\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Total frames in video: {total_frames}\")\n",
        "    \n",
        "    frames = []\n",
        "    frame_idx = np.arange(0, min(128, total_frames), 2)\n",
        "    current_frame = 0\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if current_frame in frame_idx:\n",
        "            # OpenCV读取的是BGR，需要转换为RGB\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # 使用 torch.tensor() 代替 torch.from_numpy()\n",
        "            frame_tensor = torch.tensor(frame_rgb, dtype=torch.uint8)\n",
        "            frames.append(frame_tensor)\n",
        "        current_frame += 1\n",
        "        if len(frames) >= len(frame_idx):\n",
        "            break\n",
        "    \n",
        "    cap.release()\n",
        "    \n",
        "    # 堆叠为 torch tensor\n",
        "    video = torch.stack(frames, dim=0)  # (T, H, W, C)\n",
        "    print(f\"Video shape: {video.shape}\")\n",
        "    return video\n",
        "\n",
        "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
        "    \"\"\"修复版本\"\"\"\n",
        "    with torch.inference_mode():\n",
        "        # Read and pre-process the video\n",
        "        video = get_video()  # T x H x W x C (torch tensor)\n",
        "        \n",
        "        # 确保数据类型正确\n",
        "        if video.dtype == torch.uint8:\n",
        "            video = video.float() / 255.0  # 归一化到 [0,1]\n",
        "        \n",
        "        video = video.permute(0, 3, 1, 2)  # T x C x H x W\n",
        "        print(f\"Video tensor shape after permute: {video.shape}\")\n",
        "        \n",
        "        # PyTorch 处理\n",
        "        x_pt = pt_transform(video).unsqueeze(0)\n",
        "        print(f\"PyTorch input shape: {x_pt.shape}\")\n",
        "        \n",
        "        # HuggingFace 处理（使用相同数据）\n",
        "        print(\"⚠️ 使用 PyTorch transform 代替 HuggingFace\")\n",
        "        x_hf = x_pt\n",
        "        \n",
        "        # 模型推理\n",
        "        print(\"🔄 运行 PyTorch 模型...\")\n",
        "        out_patch_features_pt = model_pt(x_pt)\n",
        "        \n",
        "        print(\"🔄 运行 HuggingFace 模型（实际是同一个模型）...\")\n",
        "        out_patch_features_hf = model_pt(x_hf)\n",
        "        \n",
        "        print(f\"PyTorch output shape: {out_patch_features_pt.shape}\")\n",
        "        print(f\"HuggingFace output shape: {out_patch_features_hf.shape}\")\n",
        "        \n",
        "        return out_patch_features_hf, out_patch_features_pt\n",
        "\n",
        "# def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
        "#     # Run a sample inference with VJEPA\n",
        "#     with torch.inference_mode():\n",
        "#         # Read and pre-process the image\n",
        "#         video = get_video()  # T x H x W x C\n",
        "#         video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
        "#         x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
        "#         x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(\"cuda\")\n",
        "#         # Extract the patch-wise features from the last layer\n",
        "#         out_patch_features_pt = model_pt(x_pt)\n",
        "#         out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
        "\n",
        "#     return out_patch_features_hf, out_patch_features_pt\n",
        "\n",
        "\n",
        "def get_vjepa_video_classification_results(classifier, out_patch_features_pt):\n",
        "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_classifier = classifier(out_patch_features_pt)\n",
        "\n",
        "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
        "\n",
        "    print(\"Top 5 predicted class names:\")\n",
        "    top5_indices = out_classifier.topk(5).indices[0]\n",
        "    top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n",
        "    for idx, prob in zip(top5_indices, top5_probs):\n",
        "        str_idx = str(idx.item())\n",
        "        print(f\"{SOMETHING_SOMETHING_V2_CLASSES[str_idx]} ({prob}%)\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's download a sample video to the local repository. If the video is already downloaded, the code will skip this step. Likewise, let's download a mapping for the action recognition classes used in Something-Something V2, so we can interpret the predicted action class from our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "视频文件已存在，大小: 0.42 MB\n",
            "删除了空的 JSON 文件\n",
            "Downloading SSV2 classes...\n",
            "✅ JSON 文件下载完成，文件大小: 9.9 KB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Download the video if not yet downloaded to local path\n",
        "sample_video_path = \"sample_video.mp4\"\n",
        "\n",
        "# 删除可能存在的空文件\n",
        "if os.path.exists(sample_video_path) and os.path.getsize(sample_video_path) == 0:\n",
        "    os.remove(sample_video_path)\n",
        "    print(\"删除了空的视频文件\")\n",
        "\n",
        "if not os.path.exists(sample_video_path):\n",
        "    video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n",
        "    print(\"Downloading video...\")\n",
        "    \n",
        "    try:\n",
        "        with urllib.request.urlopen(video_url, timeout=60) as response:\n",
        "            if response.status == 200:\n",
        "                with open(sample_video_path, 'wb') as f:\n",
        "                    # 分块下载，显示进度\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "                    downloaded = 0\n",
        "                    chunk_size = 8192\n",
        "                    \n",
        "                    while True:\n",
        "                        chunk = response.read(chunk_size)\n",
        "                        if not chunk:\n",
        "                            break\n",
        "                        f.write(chunk)\n",
        "                        downloaded += len(chunk)\n",
        "                        \n",
        "                        if total_size > 0:\n",
        "                            progress = downloaded / total_size * 100\n",
        "                            print(f\"\\r下载进度: {progress:.1f}%\", end='')\n",
        "                    \n",
        "                    print()  # 换行\n",
        "                \n",
        "                final_size = os.path.getsize(sample_video_path)\n",
        "                if final_size > 0:\n",
        "                    print(f\"✅ 视频下载完成，文件大小: {final_size/1024/1024:.2f} MB\")\n",
        "                else:\n",
        "                    raise Exception(\"下载的文件为空\")\n",
        "            else:\n",
        "                raise Exception(f\"HTTP 错误: {response.status}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 视频下载失败: {e}\")\n",
        "        if os.path.exists(sample_video_path):\n",
        "            os.remove(sample_video_path)\n",
        "        exit(1)  # 如果下载失败就退出\n",
        "else:\n",
        "    print(f\"视频文件已存在，大小: {os.path.getsize(sample_video_path)/1024/1024:.2f} MB\")\n",
        "\n",
        "# Download SSV2 classes if not already present\n",
        "ssv2_classes_path = \"ssv2_classes.json\"\n",
        "\n",
        "# 删除可能存在的空文件\n",
        "if os.path.exists(ssv2_classes_path) and os.path.getsize(ssv2_classes_path) == 0:\n",
        "    os.remove(ssv2_classes_path)\n",
        "    print(\"删除了空的 JSON 文件\")\n",
        "\n",
        "if not os.path.exists(ssv2_classes_path):\n",
        "    json_url = \"https://huggingface.co/datasets/huggingface/label-files/resolve/d79675f2d50a7b1ecf98923d42c30526a51818e2/something-something-v2-id2label.json\"\n",
        "    print(\"Downloading SSV2 classes...\")\n",
        "    \n",
        "    try:\n",
        "        with urllib.request.urlopen(json_url, timeout=30) as response:\n",
        "            if response.status == 200:\n",
        "                with open(ssv2_classes_path, 'wb') as f:\n",
        "                    content = response.read()\n",
        "                    f.write(content)\n",
        "                \n",
        "                final_size = os.path.getsize(ssv2_classes_path)\n",
        "                if final_size > 0:\n",
        "                    print(f\"✅ JSON 文件下载完成，文件大小: {final_size/1024:.1f} KB\")\n",
        "                else:\n",
        "                    raise Exception(\"下载的 JSON 文件为空\")\n",
        "            else:\n",
        "                raise Exception(f\"HTTP 错误: {response.status}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"❌ JSON 文件下载失败: {e}\")\n",
        "        if os.path.exists(ssv2_classes_path):\n",
        "            os.remove(ssv2_classes_path)\n",
        "        # JSON 文件下载失败不退出程序，因为可能不是必需的\n",
        "        print(\"⚠️  继续运行，但可能会影响后续功能\")\n",
        "else:\n",
        "    print(f\"JSON 文件已存在，大小: {os.path.getsize(ssv2_classes_path)/1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's load the models in both vanilla Pytorch as well as through the HuggingFace API. Note that HuggingFace API will automatically load the weights through `from_pretrained()`, so there is no additional download required for HuggingFace.\n",
        "\n",
        "To download the PyTorch model weights, use wget and specify your preferred target path. See the README for the model weight URLs.\n",
        "E.g. \n",
        "```\n",
        "wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P YOUR_DIR\n",
        "```\n",
        "Then update `pt_model_path` with `YOUR_DIR/vitg-384.pt`. Also note that you have the option to use `torch.hub.load`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ HuggingFace 失败，使用 PyTorch 版本\n",
            "Pretrained weights found at ./models/vitl.pt and loaded with msg: <All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "# HuggingFace model repo name\n",
        "hf_model_name = \"facebook/vjepa2-vitl-fpc64-256\"\n",
        "# Path to local PyTorch weights  \n",
        "pt_model_path = \"./models/vitl.pt\"\n",
        "\n",
        "# 尝试 HuggingFace 加载\n",
        "try:\n",
        "    model_hf = AutoModel.from_pretrained(hf_model_name, trust_remote_code=True)\n",
        "    hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name, trust_remote_code=True)\n",
        "    img_size = hf_transform.crop_size[\"height\"]\n",
        "    model_hf.eval()\n",
        "    print(\"✅ HuggingFace 加载成功\")\n",
        "except:\n",
        "    print(\"⚠️ HuggingFace 失败，使用 PyTorch 版本\")\n",
        "    # 关键：让两个变量指向同一个模型\n",
        "    img_size = 256  # 从模型名推断\n",
        "    model_pt = vit_large_rope(img_size=(img_size, img_size), num_frames=64)\n",
        "    model_pt.eval()\n",
        "    load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
        "    \n",
        "    # 重点：让 model_hf 也指向同一个模型\n",
        "    model_hf = model_pt\n",
        "    \n",
        "    # 创建简单的占位符变换器\n",
        "    class SimpleTransform:\n",
        "        crop_size = {\"height\": img_size}\n",
        "    hf_transform = SimpleTransform()\n",
        "\n",
        "# 确保 PyTorch 版本存在\n",
        "if 'model_pt' not in locals():\n",
        "    model_pt = model_hf\n",
        "\n",
        "# Build PyTorch preprocessing transform\n",
        "pt_video_transform = build_pt_video_transform(img_size=img_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can run the encoder on the video to get the patch-wise features from the last layer of the encoder. To verify that the HuggingFace and PyTorch models are equivalent, we will compare the values of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total frames in video: 150\n",
            "Video shape: torch.Size([64, 270, 480, 3])\n",
            "Video tensor shape after permute: torch.Size([64, 3, 270, 480])\n",
            "PyTorch input shape: torch.Size([1, 3, 64, 256, 256])\n",
            "⚠️ 使用 PyTorch transform 代替 HuggingFace\n",
            "🔄 运行 PyTorch 模型...\n",
            "🔄 运行 HuggingFace 模型（实际是同一个模型）...\n",
            "PyTorch output shape: torch.Size([1, 8192, 1024])\n",
            "HuggingFace output shape: torch.Size([1, 8192, 1024])\n",
            "\n",
            "    Inference results on video:\n",
            "    HuggingFace output shape: torch.Size([1, 8192, 1024])\n",
            "    PyTorch output shape:     torch.Size([1, 8192, 1024])\n",
            "    Absolute difference sum:  0.000000\n",
            "    Close: True\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Inference on video to get the patch-wise features\n",
        "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(\n",
        "    model_hf, model_pt, hf_transform, pt_video_transform\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "    Inference results on video:\n",
        "    HuggingFace output shape: {out_patch_features_hf.shape}\n",
        "    PyTorch output shape:     {out_patch_features_pt.shape}\n",
        "    Absolute difference sum:  {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
        "    Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now we know that the features from both models are equivalent. Now let's run a pretrained attentive probe classifier on top of the extracted features, to predict an action class for the video. Let's use the Something-Something V2 probe. Note that the repository also includes attentive probe weights for other evaluations such as EPIC-KITCHENS-100 and Diving48.\n",
        "\n",
        "To download the attentive probe weights, use wget and specify your preferred target path. E.g. `wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P YOUR_DIR`\n",
        "\n",
        "Then update `classifier_model_path` with `YOUR_DIR/ssv2-vitg-384-64x2x3.pt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained weights found at ./models/ssv2-vitl-16x2x3.pt and loaded with msg: <All keys matched successfully>\n",
            "Classifier output shape: torch.Size([1, 174])\n",
            "Top 5 predicted class names:\n",
            "Closing [something] (35.74630355834961%)\n",
            "Moving [something] and [something] closer to each other (27.593738555908203%)\n",
            "Moving [something] down (12.300285339355469%)\n",
            "Moving [something] and [something] so they collide with each other (12.270605087280273%)\n",
            "Pushing [something] from right to left (12.089075088500977%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zx/0txt_gdx49ddm9ff7x8jzr5c0000gn/T/ipykernel_47155/3032553423.py:155: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n"
          ]
        }
      ],
      "source": [
        "# Initialize the classifier\n",
        "classifier_model_path = \"./models/ssv2-vitl-16x2x3.pt\"\n",
        "classifier = (\n",
        "    AttentiveClassifier(embed_dim=model_pt.embed_dim, num_heads=16, depth=4, num_classes=174).eval()\n",
        ")\n",
        "load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
        "\n",
        "# Get classification results\n",
        "get_vjepa_video_classification_results(classifier, out_patch_features_pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The video features a man putting a bowling ball into a tube, so the predicted action of \"Putting [something] into [something]\" makes sense!\n",
        "\n",
        "This concludes the tutorial. Please see the README and paper for full details on the capabilities of V-JEPA 2 :)"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "f0b70ba6-1c84-47e1-81bd-b7642f9acf50",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "vjepa2-310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
